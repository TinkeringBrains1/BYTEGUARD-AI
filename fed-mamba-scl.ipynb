{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13324648,"sourceType":"datasetVersion","datasetId":8433638}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Definitive Installation for PyTorch and Mamba in Colab/Kaggle ---\n# This process ensures full compatibility between the libraries.\n\n# 1. Completely uninstall existing versions to prevent conflicts.\n# The '-y' flag automatically confirms the uninstall.\nprint(\"--- Step 1: Uninstalling existing torch and mamba libraries ---\")\n!pip uninstall -y torch torchvision torchaudio mamba-ssm causal-conv1d\n\n# 2. Install a specific, known-good version of PyTorch compatible with Colab's CUDA drivers.\nprint(\"\\n--- Step 2: Installing a compatible version of PyTorch ---\")\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# 3. Re-install Mamba from source. This will now compile against the new PyTorch version.\nprint(\"\\n--- Step 3: Compiling and installing Mamba from source ---\")\n!git clone https://github.com/state-spaces/mamba /content/mamba\n%cd /content/mamba\n!pip install .\n%cd /content/\nprint(\"DONE!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mamba-ssm[causal-conv1d] --no-build-isolation\n!pip install triton","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip3 install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\nimport numpy as np\nimport copy\nimport os\nimport pickle\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport datetime\n\n# --- 0. Environment Check ---\ntry:\n    from mamba_ssm import Mamba\n    print(\"Mamba-SSM library detected successfully.\")\nexcept ImportError:\n    print(\"CRITICAL ERROR: 'mamba_ssm' not found.\")\n    print(\"Please run the installation script provided earlier (git clone + pip install .).\")\n\n# --- 1. Configuration (Upscaled for ~35M Parameters) ---\nNUM_CLIENTS = 5\nROUNDS = 5\nLOCAL_EPOCHS = 2\nBATCH_SIZE = 16          # Reduced to 16 to fit the larger model in memory safely\nLEARNING_RATE = 5e-5     # Lower LR is safer for larger models\nMAX_SEQ_LENGTH = 128\nEMBEDDING_DIM = 640      \nVOCAB_SIZE = 50257\nNUM_CONTEXT_FEATURES = 1\nLAMBDA_SCL = 0.5\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:27:46.169038Z","iopub.execute_input":"2025-12-03T11:27:46.169556Z","iopub.status.idle":"2025-12-03T11:27:46.179620Z","shell.execute_reply.started":"2025-12-03T11:27:46.169524Z","shell.execute_reply":"2025-12-03T11:27:46.178892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. The Novelty: Supervised Contrastive Loss (SupConLoss) ---\nclass SupConLoss(nn.Module):\n    \"\"\"\n    Supervised Contrastive Learning Loss.\n    Encourages the model to pull embeddings of the same class together\n    and push embeddings of different classes apart in vector space.\n    \"\"\"\n    def __init__(self, temperature=0.07):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, features, labels):\n        features = F.normalize(features, dim=1)\n        batch_size = features.shape[0]\n        labels = labels.view(-1, 1)\n\n        # Mask: 1 if labels match, 0 if not\n        mask = torch.eq(labels, labels.T).float().to(device)\n\n        # Similarity matrix\n        anchor_dot_contrast = torch.div(\n            torch.matmul(features, features.T),\n            self.temperature\n        )\n\n        # Numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n\n        # Mask out self-contrast\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size).view(-1, 1).to(device),\n            0\n        )\n        mask = mask * logits_mask\n\n        # Compute Log-Likelihood\n        exp_logits = torch.exp(logits) * logits_mask\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-6)\n\n        # Compute mean\n        mask_sum = mask.sum(1)\n        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)\n        \n        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_sum\n\n        loss = - mean_log_prob_pos\n        loss = loss.mean()\n        return loss\n\n# --- 3. The Novel Architecture: Fed-Mamba-SCL (Larger) ---\nclass FedMambaSCL(nn.Module):\n    def __init__(self, vocab_size, num_context_features):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n        \n        # Backbone: Mamba (State Space Model)\n        # Increased d_state and expand for a \"deeper\" brain\n        self.mamba = Mamba(\n            d_model=EMBEDDING_DIM, \n            d_state=64,   # Doubled from 32\n            d_conv=4, \n            expand=4\n        )\n        \n        # Head 1: The Projector (Contrastive)\n        # Scaled up linear layers to match embedding dim\n        self.projector = nn.Sequential(\n            nn.Linear(EMBEDDING_DIM + num_context_features, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128) # Larger projection vector\n        )\n        \n        # Head 2: The Classifier (Prediction)\n        self.classifier = nn.Sequential(\n            nn.Linear(EMBEDDING_DIM + num_context_features, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input_ids, context_features):\n        embedded_seq = self.embedding(input_ids)\n        mamba_output = self.mamba(embedded_seq)\n        \n        sequence_summary = mamba_output[:, -1, :]\n        \n        combined_features = torch.cat((sequence_summary, context_features), dim=1)\n        \n        proj = self.projector(combined_features)\n        pred = self.classifier(combined_features)\n        \n        return proj, pred.squeeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:34:50.683125Z","iopub.execute_input":"2025-12-03T10:34:50.683639Z","iopub.status.idle":"2025-12-03T10:34:50.693650Z","shell.execute_reply.started":"2025-12-03T10:34:50.683615Z","shell.execute_reply":"2025-12-03T10:34:50.692757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Data Preparation ---\nclass SqlSsmDataset(Dataset):\n    def __init__(self, encodings, context_features, labels):\n        self.encodings = encodings\n        self.context_features = torch.tensor(context_features, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.float32)\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['context_features'] = self.context_features[idx]\n        item['labels'] = self.labels[idx]\n        return item\n    def __len__(self):\n        return len(self.labels)\n\ndef get_client_datasets(filepath, num_clients):\n    print(f\"Loading data from '{filepath}' and partitioning for {num_clients} clients...\")\n    df = pd.read_csv(filepath)\n    df = df.sample(frac=1).reset_index(drop=True)\n    client_dfs = np.array_split(df, num_clients)\n    client_data = []\n    \n    global_scaler = StandardScaler()\n    all_context = df['flow_duration'].values.reshape(-1, 1)\n    global_scaler.fit(all_context)\n    \n    with open(\"global_scaler.pkl\", \"wb\") as f:\n        pickle.dump(global_scaler, f)\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n\n    for i, local_df in enumerate(client_dfs):\n        texts = local_df['text'].astype(str).tolist()\n        context = local_df['flow_duration'].values.reshape(-1, 1)\n        labels = local_df['label'].values\n        \n        context_scaled = global_scaler.transform(context)\n        encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=MAX_SEQ_LENGTH, return_tensors='pt')\n        \n        dataset = SqlSsmDataset(encodings, context_scaled, labels)\n        client_data.append(dataset)\n        print(f\"  - Client {i+1} prepared: {len(dataset)} samples.\")\n        \n    return client_data, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:34:55.503396Z","iopub.execute_input":"2025-12-03T10:34:55.504154Z","iopub.status.idle":"2025-12-03T10:34:55.511522Z","shell.execute_reply.started":"2025-12-03T10:34:55.504130Z","shell.execute_reply":"2025-12-03T10:34:55.510840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Client Training Logic ---\ndef client_update(client_model, train_loader, epochs):\n    client_model.train()\n    optimizer = AdamW(client_model.parameters(), lr=LEARNING_RATE)\n    \n    criterion_scl = SupConLoss(temperature=0.1)\n    criterion_ce = nn.BCELoss()\n    \n    for epoch in range(epochs):\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            context = batch['context_features'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            \n            proj, pred = client_model(input_ids, context)\n            \n            loss_contrastive = criterion_scl(proj, labels)\n            loss_classification = criterion_ce(pred, labels)\n            \n            total_loss = loss_classification + (LAMBDA_SCL * loss_contrastive)\n            \n            total_loss.backward()\n            optimizer.step()\n            \n    return client_model.state_dict()\n\n# --- 6. Server Aggregation ---\ndef server_aggregate(global_model, client_weights):\n    global_dict = global_model.state_dict()\n    for k in global_dict.keys():\n        all_client_tensors = torch.stack([client_weights[i][k].float() for i in range(len(client_weights))], 0)\n        global_dict[k] = torch.mean(all_client_tensors, 0)\n    global_model.load_state_dict(global_dict)\n    return global_model\n\n# --- 7. Evaluation Utility ---\ndef evaluate_global(model, test_filepath):\n    if not os.path.exists(test_filepath):\n        print(\"  ! Test file not found, skipping evaluation.\")\n        return\n\n    df_test = pd.read_csv(test_filepath)\n    texts = df_test['text'].astype(str).tolist()\n    \n    with open(\"global_scaler.pkl\", \"rb\") as f:\n        scaler = pickle.load(f)\n    context = scaler.transform(df_test['flow_duration'].values.reshape(-1, 1))\n    labels = df_test['label'].values\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n    encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=MAX_SEQ_LENGTH, return_tensors='pt')\n    \n    test_dataset = SqlSsmDataset(encodings, context, labels)\n    loader = DataLoader(test_dataset, batch_size=64)\n    \n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n    \n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch['input_ids'].to(device)\n            context = batch['context_features'].to(device)\n            lbs = batch['labels'].to(device)\n            _, preds = model(input_ids, context)\n            \n            # Store probabilities for ROC\n            probs = preds.cpu().numpy()\n            preds_binary = (preds > 0.5).long()\n            \n            all_probs.extend(probs)\n            all_preds.extend(preds_binary.cpu().numpy())\n            all_labels.extend(lbs.cpu().numpy())\n            \n    acc = accuracy_score(all_labels, all_preds)\n    print(f\"  > Global Model Accuracy on Test Set: {acc:.2%}\")\n    \n    # --- Generate ROC Curve ---\n    try:\n        fpr, tpr, _ = roc_curve(all_labels, all_probs)\n        roc_auc = auc(fpr, tpr)\n        \n        plt.figure(figsize=(8, 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (Global Model)')\n        plt.legend(loc=\"lower right\")\n        \n        plt.show() # Display the plot directly\n        plt.close()\n    except Exception as e:\n        print(f\"  > Could not generate ROC curve: {e}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:28:00.356370Z","iopub.execute_input":"2025-12-03T11:28:00.356899Z","iopub.status.idle":"2025-12-03T11:28:00.369338Z","shell.execute_reply.started":"2025-12-03T11:28:00.356874Z","shell.execute_reply":"2025-12-03T11:28:00.368578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 8. Main Orchestration Loop ---\ndef run_federated_scl():\n    print(\"=\"*50)\n    print(\"STARTING FEDERATED MAMBA SCL\")\n    print(\"=\"*50)\n    \n    if not os.path.exists('/kaggle/input/ssm-sqli/train_ssm.csv'):\n        print(\"Error: train_ssm.csv not found.\")\n        return\n        \n    client_datasets, tokenizer = get_client_datasets('/kaggle/input/ssm-sqli/train_ssm.csv', NUM_CLIENTS)\n    \n    # Initialize Larger Global Model\n    global_model = FedMambaSCL(vocab_size=len(tokenizer), num_context_features=NUM_CONTEXT_FEATURES).to(device)\n    \n    # --- Parameter Count Verification ---\n    total_params = sum(p.numel() for p in global_model.parameters() if p.requires_grad)\n    print(f\"\\n[INFO] Model Architecture Upscaled.\")\n    print(f\"[INFO] Total Trainable Parameters: {total_params:,}\")\n    if total_params < 30000000:\n        print(\"[WARNING] Parameter count is below 30M target.\")\n    else:\n        print(\"[SUCCESS] Parameter count meets >30M target.\")\n    # ------------------------------------\n\n    for round_num in range(ROUNDS):\n        print(f\"\\n--- Round {round_num + 1}/{ROUNDS} ---\")\n        local_weights = []\n        \n        for i in range(NUM_CLIENTS):\n            client_model = copy.deepcopy(global_model)\n            loader = DataLoader(client_datasets[i], batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n            \n            print(f\"  [Client {i+1}] Training locally...\")\n            w_local = client_update(client_model, loader, LOCAL_EPOCHS)\n            local_weights.append(w_local)\n            \n            del client_model\n            torch.cuda.empty_cache()\n            \n        print(\"  [Server] Aggregating weights (FedAvg)...\")\n        global_model = server_aggregate(global_model, local_weights)\n        evaluate_global(global_model, '/kaggle/input/ssm-sqli/test_ssm.csv')\n\n    final_path = \"./fed-mamba-scl-model-large\"\n    os.makedirs(final_path, exist_ok=True)\n    torch.save(global_model.state_dict(), f\"{final_path}/model_state_dict.pth\")\n    tokenizer.save_pretrained(final_path)\n    print(f\"\\nFederated Learning Complete. Saved to {final_path}\")\n\nif __name__ == \"__main__\":\n    run_federated_scl()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:36:19.207911Z","iopub.execute_input":"2025-12-03T11:36:19.208194Z","iopub.status.idle":"2025-12-03T12:05:03.090887Z","shell.execute_reply.started":"2025-12-03T11:36:19.208172Z","shell.execute_reply":"2025-12-03T12:05:03.090241Z"}},"outputs":[],"execution_count":null}]}